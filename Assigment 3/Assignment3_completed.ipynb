{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb291f2e",
   "metadata": {},
   "source": [
    "# Assignment 3, Predictive Methods – AVTEK 2025\n",
    "Health Insurance Charges – k-means clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd540b19",
   "metadata": {},
   "source": [
    "**Participants names and contributions**  \n",
    "- Name 1 – data loading, preprocessing  \n",
    "- Name 2 – k-means experiments and elbow method  \n",
    "- Name 3 – interpretation of results and report writing  \n",
    "\n",
    "_(Edit the names and roles above to match your group.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec682c",
   "metadata": {},
   "source": [
    "## Part 1 – Familiarizing and basic testing with the k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d8f7e0",
   "metadata": {},
   "source": [
    "### 1.1 Dataset from Kaggle – Health Insurance Charges\n",
    "\n",
    "In this assignment I use the **Health Insurance Charges Dataset** from Kaggle. The dataset describes how different personal factors are related to health insurance costs. Each row represents one person and contains the following columns:\n",
    "\n",
    "- **age** – age of the person in years (numeric)\n",
    "- **sex** – biological sex of the person (`male`, `female`)\n",
    "- **bmi** – body mass index, a measure of body fat based on height and weight (numeric)\n",
    "- **children** – number of dependents covered by the insurance (numeric)\n",
    "- **smoker** – whether the person is a smoker (`yes`/`no`)\n",
    "- **region** – residential area in the US (`northeast`, `northwest`, `southeast`, `southwest`)\n",
    "- **charges** – annual medical insurance cost billed to the person (numeric)\n",
    "\n",
    "The goal of this work is **not** to predict a label but to **discover groups of similar people** based on their lifestyle and health-related characteristics. For that purpose I apply the k-means clustering algorithm.\n",
    "\n",
    "#### Why this dataset is suitable for k-means\n",
    "- The dataset contains several **continuous numerical features** (age, bmi, children, charges) that are well suited for distance-based clustering.\n",
    "- Categorical features (sex, smoker, region) can be converted to numerical form using one-hot encoding.\n",
    "- The size of the dataset is modest, so k-means runs quickly in a normal laptop environment.\n",
    "- Interpreting clusters is meaningful in the real world. For example we can obtain clusters like:\n",
    "  - young non-smokers with low insurance charges\n",
    "  - middle-aged smokers with very high charges\n",
    "  - families with several children and medium charges\n",
    "\n",
    "Because k-means is sensitive to the scale of the features, I will standardize the data before running the algorithm. I also remove no rows, because the dataset does not contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2676f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Load and inspect the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Kaggle health insurance dataset\n",
    "df = pd.read_csv(\"insurance.csv\")\n",
    "\n",
    "# Show basic information\n",
    "print(\"Shape of the data:\", df.shape)\n",
    "print(\"\\nFirst rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nSummary statistics:\")\n",
    "display(df.describe(include=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c12db",
   "metadata": {},
   "source": [
    "### 1.2 First k-means run\n",
    "\n",
    "In the first experiment I run a very basic k-means clustering with **k = 3** clusters. The steps are:\n",
    "1. One-hot encode the categorical columns `sex`, `smoker` and `region`.\n",
    "2. Standardize all features so that each column has mean 0 and standard deviation 1.\n",
    "3. Run k-means with `k=3` using the default parameters of scikit-learn.\n",
    "4. Attach the cluster labels back to the original data and take a quick look at the average values inside each cluster.\n",
    "\n",
    "This first run is only to check that my notebook works and that the clusters look reasonable. Optimising the number of clusters and hyperparameters is done later in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d697a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 First basic k-means run with k = 3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# One-hot encode categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=[\"sex\", \"smoker\", \"region\"], drop_first=True)\n",
    "\n",
    "# Standardize all encoded features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_encoded)\n",
    "\n",
    "# Run k-means with k = 3\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=42)\n",
    "clusters_3 = kmeans_3.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_with_clusters = df.copy()\n",
    "df_with_clusters[\"cluster_3\"] = clusters_3\n",
    "\n",
    "# Inspect average values in each cluster\n",
    "cluster_means = df_with_clusters.groupby(\"cluster_3\")[[\"age\", \"bmi\", \"children\", \"charges\"]].mean()\n",
    "print(\"Average values in each cluster (k=3):\")\n",
    "display(cluster_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30df7f",
   "metadata": {},
   "source": [
    "#### Interpretation of the first k-means run\n",
    "The table above shows the mean age, BMI, number of children and insurance charges for each of the three clusters.\n",
    "Typical observations (your exact numbers may differ slightly):\n",
    "- One cluster contains **younger non-smokers** with relatively **low charges**.\n",
    "- Another cluster includes **middle-aged people with higher BMI** and **medium charges**.\n",
    "- The third cluster often contains **smokers** with clearly **higher average charges**.\n",
    "\n",
    "From this we see that even a simple k-means run can already separate the population into meaningful groups. Next I will discuss general real-world use cases for the k-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a5a311",
   "metadata": {},
   "source": [
    "### 1.3 Listing of 2 more interesting real-world use cases for k-means algorithm\n",
    "\n",
    "1. **Customer segmentation in marketing**  \n",
    "   Companies can cluster customers based on their purchase history, demographics and browsing behaviour.    The resulting segments (e.g. price-sensitive, loyal high spenders, occasional buyers) can be used to tailor    marketing campaigns, recommend products and design loyalty programmes.\n",
    "\n",
    "2. **Image compression and colour quantisation**  \n",
    "   In computer vision k-means can be used to reduce the number of colours in an image by clustering similar    colours together. Each cluster centre represent one colour in the compressed image. This is a practical    application where k-means helps to reduce file size while keeping the visual appearance reasonable.\n",
    "\n",
    "3. **Anomaly detection as a side effect of clustering**  \n",
    "   When k-means is used to model the normal structure of data, points that lie very far from any cluster    centre may represent anomalies. For example unusual network traffic patterns or extremely high sensor    readings could be detected using this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3560d1d0",
   "metadata": {},
   "source": [
    "## Part 2 – Experimenting with the k-means algorithm more in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce70df36",
   "metadata": {},
   "source": [
    "### 2.1 Experiments with different values of $k$\n",
    "\n",
    "In this section I experiment with **different values of k** to see how the clustering changes. I use values `k = 2, 3, 4, 5, 6`. For each k I run k-means on the same standardized data and then look at:\n",
    "- the **inertia** (sum of squared distances to the nearest cluster centre),\n",
    "- the **size of each cluster**, and\n",
    "- the **average charges** inside each cluster.\n",
    "\n",
    "The goal here is not yet to find the perfect k but to understand how the structure of the clusters changes when we change the value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add36555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Experiments with several k values\n",
    "from collections import Counter\n",
    "\n",
    "results = []\n",
    "\n",
    "for k in range(2, 7):  # k = 2..6\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Store inertia and cluster sizes\n",
    "    inertia = kmeans.inertia_\n",
    "    sizes = Counter(labels)\n",
    "    \n",
    "    # Create a small summary table of average charges per cluster\n",
    "    temp = df.copy()\n",
    "    temp[\"cluster\"] = labels\n",
    "    mean_charges = temp.groupby(\"cluster\")[\"charges\"].mean().values\n",
    "    \n",
    "    results.append((k, inertia, sizes, mean_charges))\n",
    "\n",
    "# Print results in a readable form\n",
    "for k, inertia, sizes, mean_charges in results:\n",
    "    print(f\"\\n=== k = {k} ===\")\n",
    "    print(\"Inertia:\", round(inertia, 2))\n",
    "    print(\"Cluster sizes:\", dict(sizes))\n",
    "    print(\"Mean charges in clusters:\", [round(x, 2) for x in mean_charges])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f4a430",
   "metadata": {},
   "source": [
    "#### Interpretation of experiments with different k\n",
    "From the printed results we can make the following observations (exact numbers depend on the random state):\n",
    "- When **k is small (k=2)** we only get a very rough division of the population into low/medium and high charges.\n",
    "- Increasing **k to 3 or 4** splits the population into more detailed groups, for example separating smokers with very high charges from non-smokers with medium charges.\n",
    "- With **larger k (5 or 6)** some clusters become quite small. This might indicate that we are starting to over-segment the data.\n",
    "\n",
    "Therefore, from a practical point of view, values around **k = 3 or k = 4** seem reasonable for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a175d00",
   "metadata": {},
   "source": [
    "### 2.2 Utilization for Elbow method\n",
    "\n",
    "To choose a suitable number of clusters more systematically, I apply the **Elbow method**. I compute the k-means inertia for k values from 1 to 10 and then plot the results. The idea is to look for a **\"knee\" or elbow** in the curve where the decrease in inertia becomes much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a408ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Elbow method\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_values = list(range(1, 11))\n",
    "inertias = []\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(k_values, inertias, marker=\"o\")\n",
    "plt.xlabel(\"Number of clusters k\")\n",
    "plt.ylabel(\"Inertia (sum of squared distances)\")\n",
    "plt.title(\"Elbow method for k-means on health insurance data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d4e59",
   "metadata": {},
   "source": [
    "#### Interpretation of the Elbow graph\n",
    "In the Elbow plot the inertia decreases quickly at the beginning (from k=1 to k=3 or k=4) and then the curve starts to flatten. This kind of behaviour suggests that adding more clusters after a certain point does not improve the model very much.\n",
    "\n",
    "For this dataset the elbow typically appears around **k = 3 or k = 4**. This matches the qualitative observations from Section 2.1 and supports the choice of using three or four clusters in further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f040a",
   "metadata": {},
   "source": [
    "### 2.3 Testing various options for the k-means algorithm\n",
    "\n",
    "Finally I test some different **options (hyperparameters)** of the k-means algorithm using `k = 4` as an example. According to the scikit-learn documentation the most relevant options include:\n",
    "- `init` – method for choosing the initial cluster centres (`\"k-means++\"` or `\"random\"`).\n",
    "- `n_init` – how many times the algorithm is run with different initialisations.\n",
    "- `max_iter` – maximum number of iterations for a single run.\n",
    "- `algorithm` – either the classic Lloyd algorithm or Elkan's variant (for Euclidean distances).\n",
    "\n",
    "I compare the following four configurations:\n",
    "1. Default settings (k-means++ initialisation).\n",
    "2. Random initialisation with the same `k`.\n",
    "3. Increased `n_init` (more restarts).\n",
    "4. Different `max_iter` values.\n",
    "\n",
    "For each configuration I record the inertia and the number of iterations used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8efc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Testing various k-means options\n",
    "configs = [\n",
    "    {\"name\": \"default (k-means++)\", \"params\": {\"n_clusters\": 4, \"random_state\": 42}},\n",
    "    {\"name\": \"random init\", \"params\": {\"n_clusters\": 4, \"init\": \"random\", \"random_state\": 42}},\n",
    "    {\"name\": \"higher n_init\", \"params\": {\"n_clusters\": 4, \"n_init\": 20, \"random_state\": 42}},\n",
    "    {\"name\": \"fewer max_iter\", \"params\": {\"n_clusters\": 4, \"max_iter\": 50, \"random_state\": 42}},\n",
    "]\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for cfg in configs:\n",
    "    km = KMeans(**cfg[\"params\"])\n",
    "    km.fit(X_scaled)\n",
    "    summary_rows.append({\n",
    "        \"configuration\": cfg[\"name\"],\n",
    "        \"inertia\": round(km.inertia_, 2),\n",
    "        \"n_iter\": km.n_iter_\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1159f92",
   "metadata": {},
   "source": [
    "#### Interpretation of the option experiments\n",
    "From the table we can draw several conclusions:\n",
    "- **Default k-means++ initialisation** already gives a good solution with relatively low inertia.\n",
    "- Using **random initialisation** may sometimes lead to slightly worse inertia or require more iterations, because the starting points are not as good.\n",
    "- Increasing **`n_init`** means that k-means is run multiple times with different initialisations and the best solution is kept. This can slightly reduce inertia at the cost of longer computation time.\n",
    "- Reducing **`max_iter`** can stop the algorithm before full convergence. In this dataset the default maximum number of iterations is usually enough, and lowering it rarely changes the result, but in more complex datasets it might lead to higher inertia.\n",
    "\n",
    "Overall these experiments show how the different k-means options affect the clustering result and confirm that the default configuration with `k-means++` initialisation is a reasonable starting point for this dataset."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
